{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tinynn.converter import TFLiteConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving to TFLite with proper Conv2D. \n",
    "!pip install git+https://github.com/alibaba/TinyNeuralNetwork.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Taken from https://github.com/vidursatija/BlazeFace-CoreML/blob/master/ML/blazeface.py\n",
    "class ResModule(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, stride=1):\n",
    "\t\tsuper(ResModule, self).__init__()\n",
    "\t\tself.stride = stride\n",
    "\t\tself.channel_pad = out_channels - in_channels\n",
    "\t\t# kernel size is always 3\n",
    "\t\tkernel_size = 3\n",
    "\n",
    "\t\tif stride == 2:\n",
    "\t\t\tself.max_pool = nn.MaxPool2d(kernel_size=stride, stride=stride)\n",
    "\t\t\tpadding = 0\n",
    "\t\telse:\n",
    "\t\t\tpadding = (kernel_size - 1) // 2\n",
    "\n",
    "\t\tself.convs = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels=in_channels, out_channels=in_channels, \n",
    "\t\t\t\t\t\tkernel_size=kernel_size, stride=stride, padding=padding, \n",
    "\t\t\t\t\t\tgroups=in_channels, bias=True),\n",
    "\t\t\tnn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "\t\t\t\t\t\tkernel_size=1, stride=1, padding=0, bias=True),\n",
    "\t\t)\n",
    "\n",
    "\t\tself.act = nn.ReLU(inplace=True)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tif self.stride == 2:\n",
    "\t\t\th = F.pad(x, (0, 2, 0, 2), \"constant\", 0)\n",
    "\t\t\tx = self.max_pool(x)\n",
    "\t\telse:\n",
    "\t\t\th = x\n",
    "\n",
    "\t\tif self.channel_pad > 0:\n",
    "\t\t\tx = F.pad(x, (0, 0, 0, 0, 0, self.channel_pad), \"constant\", 0)\n",
    "\n",
    "\t\treturn self.act(self.convs(h) + x)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels):\n",
    "\t\tsuper(ResBlock, self).__init__()\n",
    "\t\tlayers = [ResModule(in_channels, in_channels) for _ in range(7)]\n",
    "\n",
    "\t\tself.f = nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.f(x)\n",
    "\n",
    "\n",
    "# From https://github.com/google/mediapipe/blob/master/mediapipe/models/palm_detection.tflite\n",
    "class PalmDetector(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(PalmDetector, self).__init__()\n",
    "\n",
    "\t\tself.backbone1 = nn.Sequential(\n",
    "\t\t\tnn.ConstantPad2d((0, 1, 0, 1), value=0.0),\n",
    "\t\t\tnn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\n",
    "\t\t\tResBlock(32),\n",
    "\t\t\tResModule(32, 64, stride=2),\n",
    "\t\t\tResBlock(64),\n",
    "\t\t\tResModule(64, 128, stride=2),\n",
    "\t\t\tResBlock(128)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.backbone2 = nn.Sequential(\n",
    "\t\t\tResModule(128, 256, stride=2),\n",
    "\t\t\tResBlock(256)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.backbone3 = nn.Sequential(\n",
    "\t\t\tResModule(256, 256, stride=2),\n",
    "\t\t\tResBlock(256)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.upscale8to16 = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=2, stride=2, padding=0, bias=True),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "\t\t)\n",
    "\t\tself.scaled16add = ResModule(256, 256)\n",
    "\n",
    "\t\tself.upscale16to32 = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2, padding=0, bias=True),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t)\n",
    "\t\tself.scaled32add = ResModule(128, 128)\n",
    "\n",
    "\t\tself.class_32 = nn.Conv2d(in_channels=128, out_channels=2, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\t\tself.class_16 = nn.Conv2d(in_channels=256, out_channels=2, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\t\tself.class_8 = nn.Conv2d(in_channels=256, out_channels=6, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "\t\tself.reg_32 = nn.Conv2d(in_channels=128, out_channels=36, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\t\tself.reg_16 = nn.Conv2d(in_channels=256, out_channels=36, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\t\tself.reg_8 = nn.Conv2d(in_channels=256, out_channels=108, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tb1 = self.backbone1(x.permute(0, 3, 1, 2)) # 32x32 # \n",
    "\t\t# print(b1.size())\n",
    "\n",
    "\t\tb2 = self.backbone2(b1) # 16x16\n",
    "\t\t# print(b2.size())\n",
    "\n",
    "\t\tb3 = self.backbone3(b2) # 8x8\n",
    "\t\t# print(b3.size())\n",
    "\n",
    "\t\tb2 = self.upscale8to16(b3) + b2 # 16x16\n",
    "\t\tb2 = self.scaled16add(b2) # 16x16\n",
    "\t\t# print(b2.size())\n",
    "\n",
    "\t\tb1 = self.upscale16to32(b2) + b1 # 32x32\n",
    "\t\tb1 = self.scaled32add(b1)\n",
    "\t\t# print(b1.size())\n",
    "\n",
    "\t\tc8 = self.class_8(b3).permute(0, 2, 3, 1).reshape(-1, 384, 2)\n",
    "\t\tc16 = self.class_16(b2).permute(0, 2, 3, 1).reshape(-1, 512, 2)\n",
    "\t\tc32 = self.class_32(b1).permute(0, 2, 3, 1).reshape(-1, 2048, 2)\n",
    "\n",
    "\t\tr8 = self.reg_8(b3).permute(0, 2, 3, 1).reshape(-1, 384, 18)\n",
    "\t\tr16 = self.reg_16(b2).permute(0, 2, 3, 1).reshape(-1, 512, 18)\n",
    "\t\tr32 = self.reg_32(b1).permute(0, 2, 3, 1).reshape(-1, 2048, 18)\n",
    "\n",
    "\t\tc = torch.cat([c32, c16, c8], dim=1)\n",
    "\t\tr = torch.cat([r32, r16, r8], dim=1) # needs to be anchored\n",
    "\n",
    "\t\treturn r, c\n",
    "\n",
    "\tdef modify_finetune_layers(self):\n",
    "\t\tfor param in self.parameters():\n",
    "\t\t\tparam.requires_grad = False\n",
    "\t\tself.class_32 = nn.Conv2d(in_channels=128, out_channels=4, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\t\tself.class_16 = nn.Conv2d(in_channels=256, out_channels=4, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\t\tself.class_8 = nn.Conv2d(in_channels=256, out_channels=12, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "\n",
    "\tdef load_weights(self, path):\n",
    "\t    self.load_state_dict(torch.load(path))\n",
    "\t    self.eval()\n",
    "\t\n",
    "\tdef save_weights(self, path = \"palmdetector_finetuned.pth\"):\n",
    "\t\ttorch.save(self.state_dict(), path)\n",
    "\n",
    "\tdef save_to_tflite(self, path = 'palm_detection_finetuned.tflite'):\n",
    "\t\t# Uses the converter from https://github.com/alibaba/TinyNeuralNetwork. \n",
    "\t\t# pip install git+https://github.com/alibaba/TinyNeuralNetwork.git\n",
    "\t\t# The reason to use this over converting to onnx and then to tensorflow pb and then to tflite is\n",
    "\t\t# that this converter takes care of converting pytorch's NCWH to NWHC. \n",
    "\t\t# Pytorch uses NCWH for optimzied GPU training, while TfLite only supports NWHC for Conv2D operation. \n",
    "\t\t# Converting NCWH will cause a transpose before convolution and another transpose after convolution. \n",
    "\t\t# This causes the model to inference very very slow. \n",
    "\t\tx = torch.rand(1, 3, 256, 256)\n",
    "\t\tconverter = TFLiteConverter(m, x, path, input_transpose=True)\n",
    "\t\tconverter.convert()\n",
    "\n",
    "\tdef save_to_onnx(self, path = 'palm_detection_finetuned'):\n",
    "\t\tonnx_path = path + '.onnx'\n",
    "\t\tx = torch.autograd.Variable(torch.FloatTensor(torch.rand(1, 256, 256, 3)))\n",
    "\t\ttorch.onnx.export(\n",
    "\t\t\tself, \n",
    "\t\t\tx, \n",
    "\t\t\tonnx_path, \n",
    "\t\t\texport_params=True, \n",
    "\t\t\tinput_names=['input'], \n",
    "\t\t\toutput_names=['classifier', 'regressor'], \n",
    "\t\t\t# opset_version=9, \n",
    "\t\t\tdo_constant_folding=False\n",
    "\t\t)\n",
    "\t\tprint('Saved onnx at ' + onnx_path)\n",
    "\n",
    "\tdef load_anchors(self, path):\n",
    "\t    self.anchors = torch.tensor(np.load(path), dtype=torch.float32)\n",
    "\t    assert(self.anchors.ndimension() == 2)\n",
    "\t    assert(self.anchors.shape[0] == 2944)\n",
    "\t    assert(self.anchors.shape[1] == 4)\n",
    "\n",
    "\tdef _preprocess(self, x):\n",
    "\t    \"\"\"Converts the image pixels to the range [-1, 1].\"\"\"\n",
    "\t    return x.float() / 127.5 - 1.0\n",
    "\n",
    "\tdef _tensors_to_detections(self, raw_box_tensor, raw_score_tensor, anchors):\n",
    "\t    detection_boxes = self._decode_boxes(raw_box_tensor, anchors)\n",
    "\t    \n",
    "\t    thresh = 100\n",
    "\t    raw_score_tensor = raw_score_tensor.clamp(-thresh, thresh)\n",
    "\t    detection_scores = raw_score_tensor.sigmoid().squeeze(dim=-1)\n",
    "\t    \n",
    "\t    # Note: we stripped off the last dimension from the scores tensor\n",
    "\t    # because there is only has one class. Now we can simply use a mask\n",
    "\t    # to filter out the boxes with too low confidence.\n",
    "\t    mask = detection_scores >= 0.7\n",
    "\n",
    "\t    # Because each image from the batch can have a different number of\n",
    "\t    # detections, process them one at a time using a loop.\n",
    "\t    output_detections = []\n",
    "\t    for i in range(raw_box_tensor.shape[0]):\n",
    "\t        boxes = detection_boxes[i, mask[i]]\n",
    "\t        scores = detection_scores[i, mask[i]].unsqueeze(dim=-1)\n",
    "\t        output_detections.append(torch.cat((boxes, scores), dim=-1))\n",
    "\n",
    "\t    return output_detections\n",
    "\n",
    "\tdef predict_on_image(self, img):\n",
    "\t    \"\"\"Makes a prediction on a single image.\n",
    "\t    Arguments:\n",
    "\t        img: a NumPy array of shape (H, W, 3) or a PyTorch tensor of\n",
    "\t             shape (3, H, W). The image's height and width should be \n",
    "\t             128 pixels.\n",
    "\t    Returns:\n",
    "\t        A tensor with face detections.\n",
    "\t    \"\"\"\n",
    "\t    if isinstance(img, np.ndarray):\n",
    "\t        img = torch.from_numpy(img).permute((2, 0, 1))\n",
    "\n",
    "\t    return self.predict_on_batch(img.unsqueeze(0))\n",
    "\n",
    "\tdef predict_on_batch(self, x):\n",
    "\t    \"\"\"Makes a prediction on a batch of images.\n",
    "\t    Arguments:\n",
    "\t        x: a NumPy array of shape (b, H, W, 3) or a PyTorch tensor of\n",
    "\t           shape (b, 3, H, W). The height and width should be 128 pixels.\n",
    "\t    Returns:\n",
    "\t        A list containing a tensor of face detections for each image in \n",
    "\t        the batch. If no faces are found for an image, returns a tensor\n",
    "\t        of shape (0, 17).\n",
    "\t    Each face detection is a PyTorch tensor consisting of 17 numbers:\n",
    "\t        - ymin, xmin, ymax, xmax\n",
    "\t        - x,y-coordinates for the 6 keypoints\n",
    "\t        - confidence score\n",
    "\t    \"\"\"\n",
    "\t    if isinstance(x, np.ndarray):\n",
    "\t        x = torch.from_numpy(x).permute((0, 3, 1, 2))\n",
    "\n",
    "\t    assert x.shape[1] == 3\n",
    "\t    assert x.shape[2] == 256\n",
    "\t    assert x.shape[3] == 256\n",
    "\n",
    "\t    # 1. Preprocess the images into tensors:\n",
    "\t    # x = x.to(self._device())\n",
    "\t    x = self._preprocess(x)\n",
    "\n",
    "\t    # 2. Run the neural network:\n",
    "\t    with torch.no_grad():\n",
    "\t        out = self.__call__(x)\n",
    "\n",
    "\t    # 3. Postprocess the raw predictions:\n",
    "\t    detections = self._tensors_to_detections(out[1], out[0], self.anchors)\n",
    "\n",
    "\t    # 4. Non-maximum suppression to remove overlapping detections:\n",
    "\t    filtered_detections = []\n",
    "\t    for i in range(len(detections)):\n",
    "\t        faces = self._weighted_non_max_suppression(detections[i])\n",
    "\t        if len(faces) > 0:\n",
    "\t\t        faces = torch.stack(faces)\n",
    "\t\t        filtered_detections.append(faces)\n",
    "\n",
    "\t    return filtered_detections\n",
    "\n",
    "\tdef _decode_boxes(self, raw_boxes, anchors):\n",
    "\t    \"\"\"Converts the predictions into actual coordinates using\n",
    "\t    the anchor boxes. Processes the entire batch at once.\n",
    "\t    \"\"\"\n",
    "\t    boxes = torch.zeros_like(raw_boxes)\n",
    "\n",
    "\t    x_center = raw_boxes[..., 0] / 256 * anchors[:, 2] + anchors[:, 0]\n",
    "\t    y_center = raw_boxes[..., 1] / 256 * anchors[:, 3] + anchors[:, 1]\n",
    "\n",
    "\t    w = raw_boxes[..., 2] / 256 * anchors[:, 2] * 2.6\n",
    "\t    h = raw_boxes[..., 3] / 256 * anchors[:, 3] * 2.6\n",
    "\n",
    "\t    y_center = y_center - h / 5.2\n",
    "\n",
    "\t    boxes[..., 0] = x_center - w / 2.  # ymin\n",
    "\t    boxes[..., 1] = y_center - h / 2.  # xmin\n",
    "\t    boxes[..., 2] = x_center + w / 2.  # ymax\n",
    "\t    boxes[..., 3] = y_center + h / 2.  # xmax\n",
    "\n",
    "\t    for k in range(7):\n",
    "\t        offset = 4 + k*2\n",
    "\t        keypoint_x = raw_boxes[..., offset    ] / 256 * anchors[:, 2] + anchors[:, 0]\n",
    "\t        keypoint_y = raw_boxes[..., offset + 1] / 256 * anchors[:, 3] + anchors[:, 1]\n",
    "\t        boxes[..., offset    ] = keypoint_x\n",
    "\t        boxes[..., offset + 1] = keypoint_y\n",
    "\n",
    "\t    return boxes\n",
    "\n",
    "\tdef _weighted_non_max_suppression(self, detections):\n",
    "\t    \"\"\"The alternative NMS method as mentioned in the BlazeFace paper:\n",
    "\t    \"We replace the suppression algorithm with a blending strategy that\n",
    "\t    estimates the regression parameters of a bounding box as a weighted\n",
    "\t    mean between the overlapping predictions.\"\n",
    "\t    The original MediaPipe code assigns the score of the most confident\n",
    "\t    detection to the weighted detection, but we take the average score\n",
    "\t    of the overlapping detections.\n",
    "\t    The input detections should be a Tensor of shape (count, 17).\n",
    "\t    Returns a list of PyTorch tensors, one for each detected face.\n",
    "\t    \n",
    "\t    This is based on the source code from:\n",
    "\t    mediapipe/calculators/util/non_max_suppression_calculator.cc\n",
    "\t    mediapipe/calculators/util/non_max_suppression_calculator.proto\n",
    "\t    \"\"\"\n",
    "\t    if len(detections) == 0: return []\n",
    "\n",
    "\t    output_detections = []\n",
    "\n",
    "\t    # Sort the detections from highest to lowest score.\n",
    "\t    remaining = torch.argsort(detections[:, 18], descending=True)\n",
    "\n",
    "\t    while len(remaining) > 0:\n",
    "\t        detection = detections[remaining[0]]\n",
    "\n",
    "\t        # Compute the overlap between the first box and the other \n",
    "\t        # remaining boxes. (Note that the other_boxes also include\n",
    "\t        # the first_box.)\n",
    "\t        first_box = detection[:4]\n",
    "\t        other_boxes = detections[remaining, :4]\n",
    "\t        ious = overlap_similarity(first_box, other_boxes)\n",
    "\n",
    "\t        # If two detections don't overlap enough, they are considered\n",
    "\t        # to be from different faces.\n",
    "\t        mask = ious >= 0.3\n",
    "\t        overlapping = remaining[mask]\n",
    "\t        remaining = remaining[~mask]\n",
    "\n",
    "\t        # Take an average of the coordinates from the overlapping\n",
    "\t        # detections, weighted by their confidence scores.\n",
    "\t        weighted_detection = detection.clone()\n",
    "\t        if len(overlapping) > 1:\n",
    "\t            coordinates = detections[overlapping, :18]\n",
    "\t            scores = detections[overlapping, 18:19]\n",
    "\t            total_score = scores.sum()\n",
    "\t            weighted = (coordinates * scores).sum(dim=0) / total_score\n",
    "\t            weighted_detection[:18] = weighted\n",
    "\t            weighted_detection[18] = total_score / len(overlapping)\n",
    "\n",
    "\t        output_detections.append(weighted_detection)\n",
    "\n",
    "\t    return output_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PalmDetector()\n",
    "m.modify_finetune_layers()\n",
    "m.load_weights(\"./palmdetector_finetuned.pth\")\n",
    "m.load_anchors('./anchors.npy')\n",
    "# m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PalmDetector(\n",
       "  (backbone1): Sequential(\n",
       "    (0): ConstantPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "    (1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ResBlock(\n",
       "      (f): Sequential(\n",
       "        (0): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): ResModule(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), groups=32)\n",
       "        (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (f): Sequential(\n",
       "        (0): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): ResModule(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), groups=64)\n",
       "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): ResBlock(\n",
       "      (f): Sequential(\n",
       "        (0): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (backbone2): Sequential(\n",
       "    (0): ResModule(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128)\n",
       "        (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (f): Sequential(\n",
       "        (0): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (backbone3): Sequential(\n",
       "    (0): ResModule(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256)\n",
       "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (f): Sequential(\n",
       "        (0): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): ResModule(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (upscale8to16): Sequential(\n",
       "    (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (scaled16add): ResModule(\n",
       "    (convs): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (upscale16to32): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (scaled32add): ResModule(\n",
       "    (convs): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "      (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (class_32): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (class_16): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (class_8): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (reg_32): Conv2d(128, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (reg_16): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (reg_8): Conv2d(256, 108, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 256, 4, 257] to have 3 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20620/1402691624.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_to_tflite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../models/palm_detection_torch_finetuned.tflite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20620/2002215582.py\u001b[0m in \u001b[0;36msave_to_tflite\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFLiteConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_transpose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                 \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0msave_to_onnx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'palm_detection_finetuned'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\tinynn\\converter\\base.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \"\"\"\n\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_input_transpose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_jit_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_lowered_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_common_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\tinynn\\converter\\base.py\u001b[0m in \u001b[0;36minit_jit_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mscript\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[1;31m# Have to save it once, otherwise something weird happens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\jit\\_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m         return trace_module(\n\u001b[0m\u001b[0;32m    742\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[1;34m\"forward\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\jit\\_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[1;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    956\u001b[0m             \u001b[0mexample_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m             module._c._create_method_from_trace(\n\u001b[0m\u001b[0;32m    959\u001b[0m                 \u001b[0mmethod_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20620/2002215582.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 32x32 #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[1;31m# print(b1.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1088\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 256, 4, 257] to have 3 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "m.save_to_tflite('../models/palm_detection_torch_finetuned.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved onnx at palm_detection_finetuned.onnx\n"
     ]
    }
   ],
   "source": [
    "m.save_to_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (absl) Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: palm_detection_finetuned\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (tensorflow) Assets written to: palm_detection_finetuned\\assets\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load('palm_detection_finetuned.onnx')\n",
    "tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "tf_rep.export_graph(\"palm_detection_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tflite at palm_detection_torch_finetuned_quantized_float16.tflite\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model('palm_detection_finetuned')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_path = 'palm_detection_torch_finetuned_quantized_float16' + '.tflite'\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print('Saved tflite at ' + tflite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (onnx2keras:maxpool) Unable to use `same` padding. Add ZeroPadding2D layer to fix shapes.\n",
      "WARNING (onnx2keras:add) Failed to use keras.layers.Add. Fallback to TF lambda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 2, 2]\n",
      "[0, 0, 0, 0, 0, 32, 0, 0]\n",
      "Tensor(\"Placeholder:0\", shape=(None, 64, 64, 64), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(None, 32, 64, 64), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"220\" (type Lambda).\n\nDimensions must be equal, but are 64 and 32 for '{{node 220/Add}} = AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [?,64,64,64], [?,32,64,64].\n\nCall arguments received:\n   inputs=['tf.Tensor(shape=(None, 64, 64, 64), dtype=float32)', 'tf.Tensor(shape=(None, 32, 64, 64), dtype=float32)']\n   mask=None\n   training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20620/2175627488.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0monnx_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'palm_detection_finetuned.onnx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mk_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monnx_to_keras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monnx_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange_ordering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\onnx2keras\\converter.py\u001b[0m in \u001b[0;36monnx_to_keras\u001b[1;34m(onnx_model, input_names, input_shapes, name_policy, verbose, change_ordering)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_image_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         AVAILABLE_CONVERTERS[node_type](\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mnode_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\onnx2keras\\elementwise_layers.py\u001b[0m in \u001b[0;36mconvert_elementwise_add\u001b[1;34m(node, params, layers, lambda_func, node_name, keras_name)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mlambda_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mlambda_func\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeras_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gesture\\lib\\site-packages\\onnx2keras\\elementwise_layers.py\u001b[0m in \u001b[0;36mtarget_layer\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             layer = tf.add(\n\u001b[0m\u001b[0;32m     77\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"220\" (type Lambda).\n\nDimensions must be equal, but are 64 and 32 for '{{node 220/Add}} = AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [?,64,64,64], [?,32,64,64].\n\nCall arguments received:\n   inputs=['tf.Tensor(shape=(None, 64, 64, 64), dtype=float32)', 'tf.Tensor(shape=(None, 32, 64, 64), dtype=float32)']\n   mask=None\n   training=None"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx2keras import onnx_to_keras\n",
    "\n",
    "onnx_model = onnx.load('palm_detection_finetuned.onnx')\n",
    "k_model = onnx_to_keras(onnx_model, input_names=['input'], change_ordering=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb0440c4ab902b2f3e61b00d7a3e137e45b2aad8eae5c57eb21b485188aa7ca2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('gesture': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
