# MediaPipe graph to detect/predict hand landmarks on CPU.
#
# The procedure is done in two steps:
# - locate palms/hands
# - detect landmarks for each palm/hand.
# The original Mediapipe graph tries to skip palm detection as much as possible
# by reusing previously detected/predicted landmarks for new images.
# But this modified version will detect palm every time, but still reuse
# previously detected/predicted landmarks for new images.

type: "HandLandmarkTrackingCpu"

# CPU image. (ImageFrame)
input_stream: "IMAGE:image"

# Max number of hands to detect/track. (int)
input_side_packet: "NUM_HANDS:num_hands"

# Complexity of hand landmark and palm detection models: 0 or 1. Accuracy as
# well as inference latency generally go up with the model complexity. If
# unspecified, functions as set to 1. (int)
input_side_packet: "MODEL_COMPLEXITY:model_complexity"

# Whether landmarks on the previous image should be used to help localize
# landmarks on the current image. (bool)
input_side_packet: "USE_PREV_LANDMARKS:use_prev_landmarks"

# Collection of detected/predicted hands, each represented as a list of
# landmarks. (std::vector<NormalizedLandmarkList>)
# NOTE: there will not be an output packet in the LANDMARKS stream for this
# particular timestamp if none of hands detected. However, the MediaPipe
# framework will internally inform the downstream calculators of the absence of
# this packet so that they don't wait for it unnecessarily.
output_stream: "LANDMARKS:multi_hand_landmarks"

# Collection of detected/predicted hand world landmarks.
# (std::vector<LandmarkList>)
#
# World landmarks are real-world 3D coordinates in meters with the origin in the
# center of the hand bounding box calculated from the landmarks.
#
# WORLD_LANDMARKS shares the same landmark topology as LANDMARKS. However,
# LANDMARKS provides coordinates (in pixels) of a 3D object projected onto the
# 2D image surface, while WORLD_LANDMARKS provides coordinates (in meters) of
# the 3D object itself.
output_stream: "WORLD_LANDMARKS:multi_hand_world_landmarks"

# Collection of handedness of the detected hands (i.e. is hand left or right),
# each represented as a ClassificationList proto with a single Classification
# entry. (std::vector<ClassificationList>)
# Note that handedness is determined assuming the input image is mirrored,
# i.e., taken with a front-facing/selfie camera with images flipped
# horizontally.
output_stream: "HANDEDNESS:multi_handedness"

# Extra outputs (for debugging, for instance).
# Detected palms. (std::vector<Detection>)
output_stream: "PALM_DETECTIONS:palm_detections"
# Regions of interest calculated based on landmarks.
# (std::vector<NormalizedRect>)
output_stream: "HAND_ROIS_FROM_LANDMARKS:hand_rects_from_landmarks"
# Regions of interest calculated based on palm detections.
# (std::vector<NormalizedRect>)
output_stream: "HAND_ROIS_FROM_PALM_DETECTIONS:hand_rects_from_palm_detections"


################################################# START DETECT PALM, in place of PalmDetectionCpu calculator. 
# Transforms an image into a 128x128 tensor while keeping the aspect ratio, and
# therefore may result in potential letterboxing.
node {
  calculator: "ImageToTensorCalculator"
  input_stream: "IMAGE:image"
  output_stream: "TENSORS:palm_input_tensor"
  output_stream: "LETTERBOX_PADDING:palm_letterbox_padding"
  options: {
    [mediapipe.ImageToTensorCalculatorOptions.ext] {
      output_tensor_width: 256 # 256 # 192
      output_tensor_height: 256 # 256 # 192
      keep_aspect_ratio: true
      output_tensor_float_range {
        min: 0.0
        max: 1.0
      }
      border_mode: BORDER_ZERO # was BORDER_ZERO, which equals 1
    }
  }
}

# Generates a single side packet containing a TensorFlow Lite op resolver that
# supports custom ops needed by the model used in this graph.
node {
  calculator: "TfLiteCustomOpResolverCalculator"
  output_side_packet: "opresolver"
}

# Loads the palm detection TF Lite model.
# node {
#   calculator: "PalmDetectionModelLoader"
#   input_side_packet: "MODEL_COMPLEXITY:model_complexity"
#   output_side_packet: "MODEL:model"
# }
##### START loading the model
# MediaPipe graph to load a selected palm detection TF Lite model.

# Determines path to the desired pose landmark model file.
node {
  calculator: "SwitchContainer"
  input_side_packet: "SELECT:model_complexity"
  output_side_packet: "PACKET:palm_model_path"
  options: {
    [mediapipe.SwitchContainerOptions.ext] {
      select: 1
      contained_node: {
        calculator: "ConstantSidePacketCalculator"
        options: {
          [mediapipe.ConstantSidePacketCalculatorOptions.ext]: {
            packet {
              string_value: "models/palm_detection_tf_finetuned.tflite"
            }
          }
        }
      }
      contained_node: {
        calculator: "ConstantSidePacketCalculator"
        options: {
          [mediapipe.ConstantSidePacketCalculatorOptions.ext]: {
            packet {
              string_value: "models/palm_detection_tf_finetuned.tflite"
            }
          }
        }
      }
    }
  }
}

# Loads the file in the specified path into a blob.
node {
  calculator: "LocalFileContentsCalculator"
  input_side_packet: "FILE_PATH:palm_model_path"
  output_side_packet: "CONTENTS:palm_model_blob"
  options: {
    [mediapipe.LocalFileContentsCalculatorOptions.ext]: {
      text_mode: false
    }
  }
}

# Converts the input blob into a TF Lite model.
node {
  calculator: "TfLiteModelCalculator"
  input_side_packet: "MODEL_BLOB:palm_model_blob"
  output_side_packet: "MODEL:palm_model"
}
##### END loading the model

# Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
# vector of tensors representing, for instance, detection boxes/keypoints and
# scores.
node {
  calculator: "InferenceCalculator"
  input_stream: "TENSORS:palm_input_tensor"
  output_stream: "TENSORS:detection_tensors"
  input_side_packet: "CUSTOM_OP_RESOLVER:opresolver"
  input_side_packet: "MODEL:palm_model"
  options: {
    [mediapipe.InferenceCalculatorOptions.ext] {
      delegate { xnnpack {} }
    }
  }
}

# Generates a single side packet containing a vector of SSD anchors based on
# the specification in the options.
node {
  calculator: "SsdAnchorsCalculator"
  output_side_packet: "anchors"
  options: {
    [mediapipe.SsdAnchorsCalculatorOptions.ext] {
      num_layers: 5 # 5 # 4
      min_scale: 0.1171875 # 0.1171875 # 0.1484375
      max_scale: 0.75 # 0.75 # 0.75
      input_size_width: 256 # 256 # 192
      input_size_height: 256 # 256 # 192
      anchor_offset_x: 0.5
      anchor_offset_y: 0.5
      strides: 8
      strides: 16

      # strides: 16
      # strides: 16
      strides: 32
      strides: 32
      strides: 32
      aspect_ratios: 1.0
      fixed_anchor_size: true
    }
  }
}

# Decodes the detection tensors generated by the TensorFlow Lite model, based on
# the SSD anchors and the specification in the options, into a vector of
# detections. Each detection describes a detected object.
node {
  calculator: "TensorsToDetectionsCalculator"
  input_stream: "TENSORS:detection_tensors"
  input_side_packet: "ANCHORS:anchors"
  output_stream: "DETECTIONS:unfiltered_detections"
  options: {
    [mediapipe.TensorsToDetectionsCalculatorOptions.ext] {
      num_classes: 1
      num_boxes: 2944 # 2944 # 2016
      num_coords: 18
      box_coord_offset: 0
      keypoint_coord_offset: 4
      num_keypoints: 7
      num_values_per_keypoint: 2
      sigmoid_score: true
      score_clipping_thresh: 100.0
      reverse_output_order: true

      x_scale: 256.0 # 256 # 192
      y_scale: 256.0 # 256 # 192
      w_scale: 256.0 # 256 # 192
      h_scale: 256.0 # 256 # 192
      min_score_thresh: 0.5
    }
  }
}

# Performs non-max suppression to remove excessive detections.
node {
  calculator: "NonMaxSuppressionCalculator"
  input_stream: "unfiltered_detections"
  output_stream: "filtered_detections"
  options: {
    [mediapipe.NonMaxSuppressionCalculatorOptions.ext] {
      min_suppression_threshold: 0.3
      overlap_type: INTERSECTION_OVER_UNION # was INTERSECTION_OVER_UNION, which equals 3
      algorithm: WEIGHTED # was WEIGHTED, which equals 1
    }
  }
}

# Adjusts detection locations (already normalized to [0.f, 1.f]) on the
# letterboxed image (after image transformation with the FIT scale mode) to the
# corresponding locations on the same image with the letterbox removed (the
# input image to the graph before image transformation).
node {
  calculator: "DetectionLetterboxRemovalCalculator"
  input_stream: "DETECTIONS:filtered_detections"
  input_stream: "LETTERBOX_PADDING:palm_letterbox_padding"
  output_stream: "DETECTIONS:all_palm_detections"
}
################################################# END DETECT PALM


# Makes sure there are no more detections than the provided num_hands.
node {
  calculator: "ClipDetectionVectorSizeCalculator"
  input_stream: "all_palm_detections"
  output_stream: "palm_detections"
  input_side_packet: "num_hands"
}






# When the optional input side packet "use_prev_landmarks" is either absent or
# set to true, uses the landmarks on the previous image to help localize
# landmarks on the current image.
node {
  calculator: "GateCalculator"
  input_side_packet: "ALLOW:use_prev_landmarks"
  input_stream: "prev_hand_rects_from_landmarks"
  output_stream: "gated_prev_hand_rects_from_landmarks"
  options: {
    [mediapipe.GateCalculatorOptions.ext] {
      allow: true
    }
  }
}

# Extracts image size.
node {
  calculator: "ImagePropertiesCalculator"
  input_stream: "IMAGE:image" ############### was palm_detection_image
  output_stream: "SIZE:palm_detection_image_size"
}

# Outputs each element of palm_detections at a fake timestamp for the rest of
# the graph to process. Clones the image size packet for each palm_detection at
# the fake timestamp. At the end of the loop, outputs the BATCH_END timestamp
# for downstream calculators to inform them that all elements in the vector have
# been processed.
node {
  calculator: "BeginLoopDetectionCalculator"
  input_stream: "ITERABLE:palm_detections"
  input_stream: "CLONE:palm_detection_image_size"
  output_stream: "ITEM:palm_detection"
  output_stream: "CLONE:image_size_for_palms"
  output_stream: "BATCH_END:palm_detections_timestamp"
}

# Calculates region of interest (ROI) based on the specified palm.
node {
  calculator: "PalmDetectionDetectionToRoi"
  input_stream: "DETECTION:palm_detection"
  input_stream: "IMAGE_SIZE:image_size_for_palms"
  output_stream: "ROI:hand_rect_from_palm_detection"
}

# Collects a NormalizedRect for each hand into a vector. Upon receiving the
# BATCH_END timestamp, outputs the vector of NormalizedRect at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopNormalizedRectCalculator"
  input_stream: "ITEM:hand_rect_from_palm_detection"
  input_stream: "BATCH_END:palm_detections_timestamp"
  output_stream: "ITERABLE:hand_rects_from_palm_detections"
}

# Performs association between NormalizedRect vector elements from previous
# image and rects based on palm detections from the current image. This
# calculator ensures that the output hand_rects vector doesn't contain
# overlapping regions based on the specified min_similarity_threshold.
node {
  calculator: "AssociationNormRectCalculator"
  input_stream: "hand_rects_from_palm_detections"
  input_stream: "gated_prev_hand_rects_from_landmarks"
  output_stream: "hand_rects"
  options: {
    [mediapipe.AssociationCalculatorOptions.ext] {
      min_similarity_threshold: 0.5
    }
  }
}

# Extracts image size.
node {
  calculator: "ImagePropertiesCalculator"
  input_stream: "IMAGE_CPU:image"
  output_stream: "SIZE:image_size"
}

# Outputs each element of hand_rects at a fake timestamp for the rest of the
# graph to process. Clones image and image size packets for each
# single_hand_rect at the fake timestamp. At the end of the loop, outputs the
# BATCH_END timestamp for downstream calculators to inform them that all
# elements in the vector have been processed.
node {
  calculator: "BeginLoopNormalizedRectCalculator"
  input_stream: "ITERABLE:hand_rects"
  input_stream: "CLONE:0:image"
  input_stream: "CLONE:1:image_size"
  output_stream: "ITEM:single_hand_rect"
  output_stream: "CLONE:0:image_for_landmarks"
  output_stream: "CLONE:1:image_size_for_landmarks"
  output_stream: "BATCH_END:hand_rects_timestamp"
}

# Detect hand landmarks for the specific hand rect.
# node {
#   calculator: "HandLandmarkCpu"
#   input_side_packet: "MODEL_COMPLEXITY:model_complexity"
#   input_stream: "IMAGE:image_for_landmarks"
#   input_stream: "ROI:single_hand_rect"
#   output_stream: "LANDMARKS:single_hand_landmarks"
#   output_stream: "WORLD_LANDMARKS:single_hand_world_landmarks"
#   output_stream: "HANDEDNESS:single_handedness"
# }

################################################# START HandLandmarkCpu

# Transforms a region of image into a 224x224 tensor while keeping the aspect
# ratio, and therefore may result in potential letterboxing.
node {
  calculator: "ImageToTensorCalculator"
  input_stream: "IMAGE:image_for_landmarks"
  input_stream: "NORM_RECT:single_hand_rect"
  output_stream: "TENSORS:hand_input_tensor"
  output_stream: "LETTERBOX_PADDING:hand_letterbox_padding"
  options: {
    [mediapipe.ImageToTensorCalculatorOptions.ext] {
      output_tensor_width: 224
      output_tensor_height: 224
      keep_aspect_ratio: true
      output_tensor_float_range {
        min: 0.0
        max: 1.0
      }
    }
  }
}

# Loads the hand landmark TF Lite model.
# node {
#   calculator: "HandLandmarkModelLoader"
#   input_side_packet: "MODEL_COMPLEXITY:model_complexity"
#   output_side_packet: "MODEL:hand_model"
# }
##### START loading hand landmark TF Lite model

# Determines path to the desired pose landmark model file.
node {
  calculator: "SwitchContainer"
  input_side_packet: "SELECT:model_complexity"
  output_side_packet: "PACKET:hand_model_path"
  options: {
    [mediapipe.SwitchContainerOptions.ext] {
      select: 1
      contained_node: {
        calculator: "ConstantSidePacketCalculator"
        options: {
          [mediapipe.ConstantSidePacketCalculatorOptions.ext]: {
            packet {
              string_value: "models/hand_landmark_lite.tflite"
            }
          }
        }
      }
      contained_node: {
        calculator: "ConstantSidePacketCalculator"
        options: {
          [mediapipe.ConstantSidePacketCalculatorOptions.ext]: {
            packet {
              string_value: "models/hand_landmark_lite.tflite"
            }
          }
        }
      }
    }
  }
}

# Loads the file in the specified path into a blob.
node {
  calculator: "LocalFileContentsCalculator"
  input_side_packet: "FILE_PATH:hand_model_path"
  output_side_packet: "CONTENTS:hand_model_blob"
  options: {
    [mediapipe.LocalFileContentsCalculatorOptions.ext]: {
      text_mode: false
    }
  }
}

# Converts the input blob into a TF Lite model.
node {
  calculator: "TfLiteModelCalculator"
  input_side_packet: "MODEL_BLOB:hand_model_blob"
  output_side_packet: "MODEL:hand_model"
}

##### END loading hand landmark TF lite model

# Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
# vector of tensors representing, for instance, detection boxes/keypoints and
# scores.
node {
  calculator: "InferenceCalculator"
  input_side_packet: "MODEL:hand_model"
  input_stream: "TENSORS:hand_input_tensor"
  output_stream: "TENSORS:hand_output_tensors"
  options: {
    [mediapipe.InferenceCalculatorOptions.ext] {
      delegate {
        xnnpack {}
      }
    }
  }
}

# Splits a vector of tensors to multiple vectors according to the ranges
# specified in option.
node {
  calculator: "SplitTensorVectorCalculator"
  input_stream: "hand_output_tensors"
  output_stream: "landmark_tensors"
  output_stream: "hand_flag_tensor"
  output_stream: "handedness_tensor"
  output_stream: "world_landmark_tensor"
  options: {
    [mediapipe.SplitVectorCalculatorOptions.ext] {
      ranges: { begin: 0 end: 1 }
      ranges: { begin: 1 end: 2 }
      ranges: { begin: 2 end: 3 }
      ranges: { begin: 3 end: 4 }
    }
  }
}

# Converts the hand-flag tensor into a float that represents the confidence
# score of hand presence.
node {
  calculator: "TensorsToFloatsCalculator"
  input_stream: "TENSORS:hand_flag_tensor"
  output_stream: "FLOAT:hand_presence_score"
}

# Applies a threshold to the confidence score to determine whether a hand is
# present.
node {
  calculator: "ThresholdingCalculator"
  input_stream: "FLOAT:hand_presence_score"
  output_stream: "FLAG:hand_presence"
  options: {
    [mediapipe.ThresholdingCalculatorOptions.ext] {
      threshold: 0.5
    }
  }
}

# Drops handedness tensor if hand is not present.
node {
  calculator: "GateCalculator"
  input_stream: "handedness_tensor"
  input_stream: "ALLOW:hand_presence"
  output_stream: "ensured_handedness_tensor"
}

# Converts the handedness tensor into a float that represents the classification
# score of handedness.
node {
  calculator: "TensorsToClassificationCalculator"
  input_stream: "TENSORS:ensured_handedness_tensor"
  output_stream: "CLASSIFICATIONS:single_handedness"
  options: {
    [mediapipe.TensorsToClassificationCalculatorOptions.ext] {
      top_k: 1
      label_map_path: "models/handedness.txt"
      binary_classification: true
    }
  }
}

# Drops landmarks tensors if hand is not present.
node {
  calculator: "GateCalculator"
  input_stream: "landmark_tensors"
  input_stream: "ALLOW:hand_presence"
  output_stream: "ensured_landmark_tensors"
}

# Decodes the landmark tensors into a list of landmarks, where the landmark
# coordinates are normalized by the size of the input image to the model.
node {
  calculator: "TensorsToLandmarksCalculator"
  input_stream: "TENSORS:ensured_landmark_tensors"
  output_stream: "NORM_LANDMARKS:landmarks"
  options: {
    [mediapipe.TensorsToLandmarksCalculatorOptions.ext] {
      num_landmarks: 21
      input_image_width: 224
      input_image_height: 224
      # The additional scaling factor is used to account for the Z coordinate
      # distribution in the training data.
      normalize_z: 0.4
    }
  }
}

# Adjusts landmarks (already normalized to [0.f, 1.f]) on the letterboxed hand
# image (after image transformation with the FIT scale mode) to the
# corresponding locations on the same image with the letterbox removed (hand
# image before image transformation).
node {
  calculator: "LandmarkLetterboxRemovalCalculator"
  input_stream: "LANDMARKS:landmarks"
  input_stream: "LETTERBOX_PADDING:hand_letterbox_padding"
  output_stream: "LANDMARKS:scaled_landmarks"
}

# Projects the landmarks from the cropped hand image to the corresponding
# locations on the full image before cropping (input to the graph).
node {
  calculator: "LandmarkProjectionCalculator"
  input_stream: "NORM_LANDMARKS:scaled_landmarks"
  input_stream: "NORM_RECT:single_hand_rect"
  output_stream: "NORM_LANDMARKS:single_hand_landmarks"
}

# Drops world landmarks tensors if hand is not present.
node {
  calculator: "GateCalculator"
  input_stream: "world_landmark_tensor"
  input_stream: "ALLOW:hand_presence"
  output_stream: "ensured_world_landmark_tensor"
}

# Decodes the landmark tensors into a list of landmarks, where the landmark
# coordinates are normalized by the size of the input image to the model.
node {
  calculator: "TensorsToLandmarksCalculator"
  input_stream: "TENSORS:ensured_world_landmark_tensor"
  output_stream: "LANDMARKS:unprojected_world_landmarks"
  options: {
    [mediapipe.TensorsToLandmarksCalculatorOptions.ext] {
      num_landmarks: 21
    }
  }
}

# Projects the world landmarks from the cropped hand image to the corresponding
# locations on the full image before cropping (input to the graph).
node {
  calculator: "WorldLandmarkProjectionCalculator"
  input_stream: "LANDMARKS:unprojected_world_landmarks"
  input_stream: "NORM_RECT:single_hand_rect"
  output_stream: "LANDMARKS:single_hand_world_landmarks"
}
################################################# END HandLandmarkCpu








# Collects the handedness for each single hand into a vector. Upon receiving the
# BATCH_END timestamp, outputs a vector of ClassificationList at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopClassificationListCalculator"
  input_stream: "ITEM:single_handedness"
  input_stream: "BATCH_END:hand_rects_timestamp"
  output_stream: "ITERABLE:multi_handedness"
}

# Calculate region of interest (ROI) based on detected hand landmarks to reuse
# on the subsequent runs of the graph.
node {
  calculator: "HandLandmarkLandmarksToRoi"
  input_stream: "IMAGE_SIZE:image_size_for_landmarks"
  input_stream: "LANDMARKS:single_hand_landmarks"
  output_stream: "ROI:single_hand_rect_from_landmarks"
}

# Collects a set of landmarks for each hand into a vector. Upon receiving the
# BATCH_END timestamp, outputs the vector of landmarks at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopNormalizedLandmarkListVectorCalculator"
  input_stream: "ITEM:single_hand_landmarks"
  input_stream: "BATCH_END:hand_rects_timestamp"
  output_stream: "ITERABLE:multi_hand_landmarks"
}

# Collects a set of world landmarks for each hand into a vector. Upon receiving
# the BATCH_END timestamp, outputs the vector of landmarks at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopLandmarkListVectorCalculator"
  input_stream: "ITEM:single_hand_world_landmarks"
  input_stream: "BATCH_END:hand_rects_timestamp"
  output_stream: "ITERABLE:multi_hand_world_landmarks"
}

# Collects a NormalizedRect for each hand into a vector. Upon receiving the
# BATCH_END timestamp, outputs the vector of NormalizedRect at the BATCH_END
# timestamp.
node {
  calculator: "EndLoopNormalizedRectCalculator"
  input_stream: "ITEM:single_hand_rect_from_landmarks"
  input_stream: "BATCH_END:hand_rects_timestamp"
  output_stream: "ITERABLE:hand_rects_from_landmarks"
}

# Caches hand rects calculated from landmarks, and upon the arrival of the next
# input image, sends out the cached rects with timestamps replaced by that of
# the input image, essentially generating a packet that carries the previous
# hand rects. Note that upon the arrival of the very first input image, a
# timestamp bound update occurs to jump start the feedback loop.
node {
  calculator: "PreviousLoopbackCalculator"
  input_stream: "MAIN:image"
  input_stream: "LOOP:hand_rects_from_landmarks"
  input_stream_info: {
    tag_index: "LOOP"
    back_edge: true
  }
  output_stream: "PREV_LOOP:prev_hand_rects_from_landmarks"
}
